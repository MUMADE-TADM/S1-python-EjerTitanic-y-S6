{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MUMADE-TADM/S1-python-y-S5-Titanic-/blob/main/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_kY8a5rr5qC"
      },
      "source": [
        "[Yahoo finance](https://finance.yahoo.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3zKcC8XtEnq"
      },
      "outputs": [],
      "source": [
        "!pip install -U yfinance pandas_datareader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mad_HUVxyXd"
      },
      "source": [
        "# Datos:\n",
        "\n",
        "Valor de cierre del IBEX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxaB4kY0tofH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from pandas_datareader import data as pdr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBeJELUotfza"
      },
      "outputs": [],
      "source": [
        "yf.pdr_override() # <== that's all it takes :-)\n",
        "data = pdr.get_data_yahoo(\"^IBEX\", start=\"2020-01-01\", end=\"2020-10-30\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3UO3lLZvASE"
      },
      "source": [
        "## [Bollinger Bands](https://en.wikipedia.org/wiki/Bollinger_Bands):\n",
        "\n",
        "El 97% de los valores de un activo se encuentran entre un máximo de $+1.96 \\times  \\sigma(20\\ valores\\ anteriores)$ y $-1.96 \\times \\sigma(20\\ valores\\  anteriores)$ centrados en la media de los 20 valores anteriores.\n",
        "\n",
        "(Las bandas las calcula con los 20 valores anteriores). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gz2OmhmRt4n4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "d=pd.DataFrame()\n",
        "d['R']=data['Close'] #VALORES DE CIERRE. LOS LLAMAMOS REAL(R).\n",
        "d['M']=data['Close'].rolling(20).mean() #HACEMOS LA MEDIA DE CADA 20 VALORES. \n",
        "d['+M']=d['M']+data['Close'].rolling(20).std()*1.96\n",
        "d['-M']=d['M']-data['Close'].rolling(20).std()*1.96\n",
        "d.plot(figsize=(15,5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4g6TlcVyfex"
      },
      "source": [
        "## Transformación de los datos para un problema de regresión / clasificación-\n",
        "- Datos originales $[c_0,\\ldots,c_T]$\n",
        "- Datos transformados:$[[c_0,\\ldots,c_{19}],c_{20}],\\ldots,[[c_{T-20},\\ldots,c_{T-1}],c_T]$.\n",
        "\n",
        "Cada dato está compuesto por dos partes (una independiente y otra dependiente)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTbZ91vZ9tiv"
      },
      "outputs": [],
      "source": [
        "def windowData(s,window_input=1,window_output=1,step=1):\n",
        "  X=[] \n",
        "  Y=[] #la que quiero obtener\n",
        "  #Datos necesarios\n",
        "  dn=window_input+window_output\n",
        "  #Calcular cuantos pasos completos podemos realizar. Los pasos son el tamaño de la ventana. Dividimos en tantos pasos como tengamos \n",
        "  #sin contar el último bloque [input,output]\n",
        "  ld=int((len(s)-dn)/step)*step\n",
        "  for i in range(len(s)-ld-1,len(s)-dn,step): # bucle desde el primer elemento posible hasta el máximo que podamos. \n",
        "    X.append(s[i:i+window_input]) #desde donde toque hasta el tamaño del input. \n",
        "    Y.append(s[i+window_input:i+dn]) \n",
        "  return X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkai0YWG1QiW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Escalanos los datos\n",
        "dScale = MinMaxScaler()\n",
        "ldatos=d['R'].values #ldatos.shape=(len(d['R']),)\n",
        "ldatos=ldatos.reshape((len(d['R']),1)) #ldatos.shape=(len(d['R']),1). Ponemos que cada elemento tenga su propia dimensión, por eso ponemos ,1.\n",
        "dScale.fit(ldatos)\n",
        "ldatos=dScale.transform(ldatos).reshape((len(ldatos),))\n",
        "\n",
        "#Contruimos los conjunto de datos X e Y\n",
        "X,Y=windowData(ldatos,window_input=20,window_output=1,step=1) \n",
        "X=np.array(X)\n",
        "Y=np.array(Y)\n",
        "#Lo mezclo para obtener el conjunto de datos de test y train. \n",
        "#partmos los conjuntos en entranamiento y test\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.33,shuffle=False, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora ya tenemos nuestros datos preparados para la red neuronal(Perceptron)."
      ],
      "metadata": {
        "id": "_EIOm6oXusDb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83AkZGPRo-k8"
      },
      "source": [
        "# PERCEPTRON:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4iLDwKJpKIq"
      },
      "source": [
        "## Librerías necesarias:\n",
        "- [Keras](https://keras.io/)\n",
        "- [Tensorflow](https://www.tensorflow.org/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcunIHLYpodt"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential #me permite colocar la salida de una red en la entrada de la siguiente.\n",
        "from tensorflow.keras.layers import Dense,Input\n",
        "from tensorflow.keras import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdDwd3YiqiPw"
      },
      "source": [
        "## Definición de la Red:\n",
        "\n",
        "Definimos nuestro Perceptron con las siguientes capas (**layers**):\n",
        "- Modelo secuencial [Sequential](https://keras.io/api/models/sequential/).\n",
        "- Capa de entrada: [Input](https://keras.io/api/layers/core_layers/input/).\n",
        "- Capa Densa: [Dense](https://keras.io/api/layers/core_layers/dense/). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIPQtr6Wqp3R"
      },
      "outputs": [],
      "source": [
        "perceptron=Sequential() \n",
        "perceptron.add(Input(20)) #input: cada uno de los datos de entrada que teníamos. CAPA 1.\n",
        "perceptron.add(Dense(30,activation='relu')) #La salida de la capa de la línea de arriba va a ir a una capa densa. Quiero 30 neuronas en esta capa densa y además quiero que la función de activación de las neuronas de esta capa tenga una activación relu. CAPA 2. \n",
        "perceptron.add(Dense(30,activation='relu')) #La salida de la línea anterior va a otra capa densa. CAPA 3.  \n",
        "perceptron.add(Dense(1, activation='sigmoid')) #CAPA 4. Una sóla neurona y con valores de 0 ó 1 (sigmoid).\n",
        "perceptron.summary() #Arquitectura resumen. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los nº de Param que vemos en el resultado son los w.\n"
      ],
      "metadata": {
        "id": "Uj58kuA2wZK2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_nr6THhu79F"
      },
      "source": [
        "## Compilar la red:\n",
        "Definido el **Perceptrón**, lo compilamos indicando:\n",
        "- Que optimizador utilizar (gradiente).\n",
        "- Que función de error (pérdida o **loss**) (ej. distancia cuadrática, entropía...).\n",
        "- Que métricas observar cuando lo entrenemos .\n",
        "\n",
        "Función [**compile**](https://keras.io/api/models/model_training_apis/#compile-method)\n",
        "\n",
        "Esto nos permite hacer los tensores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5A6-oOZsWnQ"
      },
      "outputs": [],
      "source": [
        "# Compilamos el modelo:\n",
        "perceptron.compile(\n",
        "    optimizer='adam', #adam es un algoritmo de cálculo de gradientes. \n",
        "    loss='mean_squared_error', #La función en este caso es el error cuadrático.\n",
        "    metrics=[\n",
        "        metrics.MeanSquaredError(name='my_mse'),\n",
        "        metrics.AUC(name='my_auc'),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1BNBUN8v4n3"
      },
      "source": [
        "## Aprender los parámetros:\n",
        "\n",
        "Realizamos el **aprendizaje** indicando:\n",
        "- Los conjuntos de datos a utilizar **X** e **Y**.\n",
        "- Que proporción de los datos utilizaremos para validar el modelo.\n",
        "- El tamaño del conjunto **batch**.\n",
        "- El número de **épocas** a realizar.\n",
        "\n",
        "Método [**fit**](https://keras.io/api/models/model_training_apis/#fit-method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJqvqoKLqrMj"
      },
      "outputs": [],
      "source": [
        "history=perceptron.fit(X,Y,validation_split=0.33, batch_size=10 ,epochs=300,verbose=0) \n",
        "#batch_size:tamaño, y epochs: nº de veces que pasa por los datos. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73crXoJAsbHr"
      },
      "outputs": [],
      "source": [
        "history.history.keys() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp9q8fWerORA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch') \n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo ideal es que train y test sean muy parecidos (se ajusten bien).\n",
        "Para generalizar mucho normalmente penalizo este ajuste."
      ],
      "metadata": {
        "id": "0TG7ZwnSykHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Construimos el predictor:"
      ],
      "metadata": {
        "id": "R7E3QB9pzAEY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE8d-FyYxZUf"
      },
      "source": [
        "## Utilizar la red:\n",
        "Utilizar el perceptron aprendido con el método [**predict**](https://keras.io/api/models/model_training_apis/#predict-method)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRzCvdbZwZ6k"
      },
      "outputs": [],
      "source": [
        "Y0=perceptron.predict(X_train) \n",
        "Y1=perceptron.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Unxa4WnnwoFo"
      },
      "outputs": [],
      "source": [
        "r=pd.DataFrame(np.vstack((Y_train,Y_test)),columns=['R'])\n",
        "r['Perceptron']=pd.DataFrame(np.vstack((Y0,Y1)))\n",
        "r.plot(figsize=(15,5))\n",
        "#Pintamos lo que da perceptron respecto al valor real. Son las predicciones sobre el conjunto de test."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se ajusta bastante bien, pero suaviza los picos y tiende a infravalorar. Además, a partir del 100 tiende a adelantarse un poco al valor real. \n",
        "\n",
        "Lo que podemos hacer para solucionar estas pequeñas cosas, es coger ventanas más pequeñas. "
      ],
      "metadata": {
        "id": "PjysDoW-zgY5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PxjNviiy-W1"
      },
      "source": [
        "## Salvar y Cargar Redes:\n",
        "- Salvar mediante el método [**save**](https://keras.io/api/models/model_saving_apis/#save-method).\n",
        "- Recuperar un modelo con el método [**load_model**](keras.models.load_model).\n",
        "\n",
        "(Guardo mi modelo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4lQ3vSOzGCx"
      },
      "outputs": [],
      "source": [
        "perceptron.save('perceptron.h5')\n",
        "otro_perceptron=keras.models.load_model('perceptron.h5')\n",
        "#LOS FICHEROS H5 SON FICHEROS JERÁRQUICOS. ESTÁN DISEÑADOS PARA ALMACENAR TENSORES DE FORMA JERÁRQUICA. "
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "NN",
      "language": "python",
      "name": "nn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}